import os
import glob
import re
from bs4 import BeautifulSoup, Comment

directory = "./my_site"  # ‚Üê –∑–º—ñ–Ω–∏, —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ

print("üî• –ü–æ—á–∏–Ω–∞—é –û–°–¢–ê–¢–û–ß–ù–ï –æ—á–∏—â–µ–Ω–Ω—è —Å–∞–π—Ç—É –≤—ñ–¥ Wayback Machine...\n")

# –†–æ–∑—à–∏—Ä–µ–Ω–∞ —Ä–µ–≥—É–ª—è—Ä–∫–∞ ‚Äî –ª–æ–≤–∏—Ç—å –í–°–Ü –º–æ–∂–ª–∏–≤—ñ Wayback-–ø—Ä–µ—Ñ—ñ–∫—Å–∏
WAYBACK_PATTERN = re.compile(
    r"https?://web\.archive\.org/web/\d{14}(?:im_|js_|cs_|if_)?/?"
    r"|/?web/\d{14}(?:im_|js_|cs_|if_)?/?"
    r"|/?(?:cs_|js_|if_)_"   # ‚Üê –ù–û–í–ï: –∫–æ—Ä–æ—Ç–∫—ñ –ø—Ä–µ—Ñ—ñ–∫—Å–∏ —Ç–∏–ø—É cs_/ js_/
)

html_files = glob.glob(os.path.join(directory, "**/*.html"), recursive=True)

cleaned = 0
fixed_urls = 0

for file_path in html_files:
    rel_path = os.path.relpath(file_path, directory)
    try:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()

        if not content.strip():
            continue

        soup = BeautifulSoup(content, "html.parser")
        modified = False

        # 1. –í–∏–¥–∞–ª—è—î–º–æ —Ç—ñ–ª—å–∫–∏ —à–∫—ñ–¥–ª–∏–≤–µ Wayback-—Å–º—ñ—Ç—Ç—è
        for tag in soup.find_all(["script", "link", "meta", "noscript"]):
            # –í–∏–¥–∞–ª—è—î–º–æ wombat, playback, ruffle —Å–∫—Ä–∏–ø—Ç–∏
            if tag.name == "script" and tag.get("src"):
                src = tag.get("src", "")
                if any(bad in src for bad in ["wombat.js", "bundle-playback.js", "ruffle", "__wm"]):
                    tag.decompose()
                    modified = True
            # –í–∏–¥–∞–ª—è—î–º–æ –±–∞–Ω–µ—Ä–Ω—ñ —Å—Ç–∏–ª—ñ Wayback
            if tag.name == "link" and tag.get("href") and "banner-styles.css" in tag.get("href", ""):
                tag.decompose()
                modified = True

        # 2. –í–∏–¥–∞–ª—è—î–º–æ –∫–æ–º–µ–Ω—Ç–∞—Ä—ñ —Ç–∞ –µ–ª–µ–º–µ–Ω—Ç–∏ Wayback
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            if any(kw in str(comment).lower() for kw in ["wayback", "archive", "playback timings", "file archived on", "wombat"]):
                comment.extract()
                modified = True

        for elem in soup.find_all(id=re.compile(r"wm-.*", re.I)):
            elem.decompose()
            modified = True

        # 3. –í–ò–ü–†–ê–í–õ–Ø–Ñ–ú–û –í–°–Ü URL (–≤–∫–ª—é—á–∞—é—á–∏ cs_/, js_/)
        for tag in soup.find_all(True):
            attrs = ["href", "src", "srcset", "xlink:href", "data-src", "content"]
            for attr in attrs:
                if tag.has_attr(attr):
                    old_value = tag[attr]

                    if attr == "srcset":
                        sources = [s.strip() for s in old_value.split(",") if s.strip()]
                        cleaned_sources = []
                        for source in sources:
                            parts = source.split()
                            url = parts[0]
                            rest = " ".join(parts[1:]) if len(parts) > 1 else ""
                            cleaned_url = WAYBACK_PATTERN.sub("", url)
                            if cleaned_url != url:
                                modified = True
                                fixed_urls += 1
                            cleaned_sources.append(f"{cleaned_url} {rest}".strip())
                        new_value = ", ".join(cleaned_sources)
                    else:
                        new_value = WAYBACK_PATTERN.sub("", old_value)

                    if new_value != old_value:
                        tag[attr] = new_value
                        fixed_urls += 1
                        modified = True

        # 4. –í–∏–ø—Ä–∞–≤–ª—è—î–º–æ –¥—É–±–ª—å–æ–≤–∞–Ω—ñ —Ç–µ–≥–∏
        for tag_name in ["html", "head", "body"]:
            tags = soup.find_all(tag_name)
            if len(tags) > 1:
                main_tag = tags[0]
                for extra in tags[1:]:
                    while extra.contents:
                        main_tag.append(extra.contents[0])
                    extra.decompose()
                modified = True

        # 5. –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø—Ä–∏ –∑–º—ñ–Ω–∞—Ö
        if modified:
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(str(soup))  # –±–µ–∑ prettify, —â–æ–± –Ω–µ –ª–∞–º–∞—Ç–∏ —Ñ–æ—Ä–º–∞—Ç (–∞–±–æ –∑ prettify, —è–∫—â–æ —Ö–æ—á–µ—à)
            cleaned += 1
            print(f"‚úì –û—á–∏—â–µ–Ω–æ: {rel_path}")

    except Exception as e:
        print(f"‚úó –ü–æ–º–∏–ª–∫–∞ –≤ {rel_path}: {e}")

print("\n" + "="*70)
print(f"–ì–û–¢–û–í–û! –û–±—Ä–æ–±–ª–µ–Ω–æ —Ñ–∞–π–ª—ñ–≤: {cleaned}")
print(f"–í–∏–ø—Ä–∞–≤–ª–µ–Ω–æ URL: {fixed_urls}")
print("="*70)
print("–¢–µ–ø–µ—Ä —Å—Ç–∏–ª—ñ (styles.css, client.css) –º–∞—é—Ç—å –∑–∞–≤–∞–Ω—Ç–∞–∂—É–≤–∞—Ç–∏—Å—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ!")
print("‚Ä¢ –ü–µ—Ä–µ–≤—ñ—Ä –ø—ñ—Å–ª—è –æ—á–∏—â–µ–Ω–Ω—è –∫–µ—à—É –±—Ä–∞—É–∑–µ—Ä–∞ (Ctrl+Shift+R)")
print("‚Ä¢ –Ø–∫—â–æ —â–æ—Å—å —â–µ –Ω–µ —Ç–∞–∫ ‚Äî —Å–∫–∏–Ω—å –Ω–æ–≤–∏–π —à–º–∞—Ç–æ–∫ <head>")
print("="*70)